{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen → Flash-JEPA: Full Knowledge Reincarnation\n",
    "\n",
    "This notebook transfers ALL of Qwen-3 into Flash-JEPA's liquid neurons.\n",
    "\n",
    "**Result**: Gen 0 Flash-JEPA = Qwen reborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers safetensors huggingface_hub accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Drive for output\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/flash-jepa/'\n",
    "!mkdir -p {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors import safe_open\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Qwen-3 Model Configuration\n",
    "MODEL_ID = \"Qwen/Qwen3-235B-A22B\"  # Or Qwen3-VL version\n",
    "print(f\"Target: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download index to find all shards\n",
    "print(\"Downloading model index...\")\n",
    "index_path = snapshot_download(repo_id=MODEL_ID, allow_patterns=[\"*.index.json\"])\n",
    "\n",
    "# Find index file\n",
    "index_file = None\n",
    "for root, dirs, files in os.walk(index_path):\n",
    "    for f in files:\n",
    "        if f.endswith('.index.json'):\n",
    "            index_file = os.path.join(root, f)\n",
    "            break\n",
    "\n",
    "with open(index_file, 'r') as f:\n",
    "    index = json.load(f)\n",
    "\n",
    "# Get unique shard files\n",
    "shards = list(set(index['weight_map'].values()))\n",
    "print(f\"Found {len(shards)} shards to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create Flash-JEPA brain structure\n",
    "# We'll store Qwen's layers as liquid neuron parameters\n",
    "\n",
    "class LiquidNeuronStore:\n",
    "    \"\"\"Stores Qwen weights as liquid neuron dynamics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embeddings = None  # Token embeddings\n",
    "        self.layers = []        # Transformer layers → liquid dynamics\n",
    "        self.lm_head = None     # Output head\n",
    "        self.config = {}\n",
    "    \n",
    "    def inject_embedding(self, weight):\n",
    "        \"\"\"Store embedding matrix.\"\"\"\n",
    "        self.embeddings = weight.clone()\n",
    "        self.config['vocab_size'] = weight.shape[0]\n",
    "        self.config['hidden_size'] = weight.shape[1]\n",
    "        print(f\"Stored embeddings: {weight.shape}\")\n",
    "    \n",
    "    def inject_layer(self, layer_idx, weights_dict):\n",
    "        \"\"\"Convert transformer layer to liquid neuron format.\"\"\"\n",
    "        liquid_layer = {\n",
    "            # Attention → Liquid recurrence\n",
    "            'W_input': None,    # Query/Key projection\n",
    "            'W_recurrent': None, # Value as recurrent connection\n",
    "            'tau': None,        # Time constants from attention weights\n",
    "            \n",
    "            # MLP → Liquid nonlinearity\n",
    "            'gate': None,       # Gate weights\n",
    "            'up': None,         # Up projection\n",
    "            'down': None,       # Down projection\n",
    "        }\n",
    "        \n",
    "        for name, weight in weights_dict.items():\n",
    "            if 'q_proj' in name or 'k_proj' in name:\n",
    "                if liquid_layer['W_input'] is None:\n",
    "                    liquid_layer['W_input'] = weight.clone()\n",
    "                else:\n",
    "                    liquid_layer['W_input'] = liquid_layer['W_input'] + weight\n",
    "            elif 'v_proj' in name:\n",
    "                liquid_layer['W_recurrent'] = weight.clone()\n",
    "            elif 'o_proj' in name:\n",
    "                # Derive tau from output projection magnitude\n",
    "                liquid_layer['tau'] = weight.abs().mean(dim=1)\n",
    "            elif 'gate' in name:\n",
    "                liquid_layer['gate'] = weight.clone()\n",
    "            elif 'up' in name:\n",
    "                liquid_layer['up'] = weight.clone()\n",
    "            elif 'down' in name:\n",
    "                liquid_layer['down'] = weight.clone()\n",
    "        \n",
    "        # Ensure we have the layer index\n",
    "        while len(self.layers) <= layer_idx:\n",
    "            self.layers.append(None)\n",
    "        self.layers[layer_idx] = liquid_layer\n",
    "        print(f\"Injected layer {layer_idx}\")\n",
    "    \n",
    "    def inject_lm_head(self, weight):\n",
    "        \"\"\"Store output head.\"\"\"\n",
    "        self.lm_head = weight.clone()\n",
    "        print(f\"Stored LM head: {weight.shape}\")\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save as Flash-JEPA brain.\"\"\"\n",
    "        brain_state = {\n",
    "            'version': 'qwen-reincarnation-1.0',\n",
    "            'config': self.config,\n",
    "            'embeddings': self.embeddings,\n",
    "            'layers': self.layers,\n",
    "            'lm_head': self.lm_head,\n",
    "            'num_layers': len([l for l in self.layers if l is not None])\n",
    "        }\n",
    "        torch.save(brain_state, path)\n",
    "        size_gb = os.path.getsize(path) / (1024**3)\n",
    "        print(f\"Saved brain to {path} ({size_gb:.2f} GB)\")\n",
    "\n",
    "brain = LiquidNeuronStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Stream and inject each shard\n",
    "print(\"Starting full knowledge transfer...\")\n",
    "\n",
    "for i, shard_name in enumerate(shards):\n",
    "    print(f\"\\n=== Processing shard {i+1}/{len(shards)}: {shard_name} ===\")\n",
    "    \n",
    "    # Download this shard\n",
    "    shard_path = snapshot_download(repo_id=MODEL_ID, allow_patterns=[shard_name])\n",
    "    shard_file = os.path.join(shard_path, shard_name)\n",
    "    \n",
    "    if not os.path.exists(shard_file):\n",
    "        # Find it\n",
    "        for root, dirs, files in os.walk(shard_path):\n",
    "            if shard_name in files:\n",
    "                shard_file = os.path.join(root, shard_name)\n",
    "                break\n",
    "    \n",
    "    # Open and extract tensors\n",
    "    with safe_open(shard_file, framework=\"pt\", device=\"cpu\") as f:\n",
    "        for key in f.keys():\n",
    "            tensor = f.get_tensor(key)\n",
    "            \n",
    "            # Route to appropriate storage\n",
    "            if 'embed_tokens' in key:\n",
    "                brain.inject_embedding(tensor)\n",
    "            elif 'lm_head' in key:\n",
    "                brain.inject_lm_head(tensor)\n",
    "            elif 'layers.' in key:\n",
    "                # Extract layer index\n",
    "                parts = key.split('.')\n",
    "                layer_idx = int(parts[parts.index('layers') + 1])\n",
    "                brain.inject_layer(layer_idx, {key: tensor})\n",
    "            \n",
    "            # Clear to save RAM\n",
    "            del tensor\n",
    "    \n",
    "    # Clear shard from disk to save space\n",
    "    # os.remove(shard_file)  # Uncomment if needed\n",
    "    print(f\"Shard {i+1} complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Save the reincarnated brain\n",
    "output_path = os.path.join(OUTPUT_DIR, 'gen_0_qwen_reincarnation.pt')\n",
    "brain.save(output_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"REINCARNATION COMPLETE!\")\n",
    "print(f\"Gen 0 Flash-JEPA saved to: {output_path}\")\n",
    "print(f\"Layers: {brain.config}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Download `gen_0_qwen_reincarnation.pt` from Drive\n",
    "2. Place in `models/saved/` folder\n",
    "3. Run `autonomous_life.py` - Gen 0 IS Qwen!\n",
    "4. Evolution will optimize over generations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
